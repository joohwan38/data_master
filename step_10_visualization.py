# step_10_visualization.py - ì‹œê°í™” ë‹´ë‹¹ (í‘œ í˜•íƒœ AI ë¶„ì„ ì¶”ê°€)

import dearpygui.dearpygui as dpg
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram
import warnings
warnings.filterwarnings('ignore')
import functools
import utils

# --- Module State ---
_module_main_callbacks: Optional[Dict] = None
_util_funcs: Optional[Dict[str, Any]] = None
_viz_tab_counter: int = 0
_texture_tags: List[str] = []

def initialize(main_callbacks: dict):
    """ì‹œê°í™” ëª¨ë“ˆ ì´ˆê¸°í™”"""
    global _module_main_callbacks, _util_funcs
    _module_main_callbacks = main_callbacks
    _util_funcs = main_callbacks.get('get_util_funcs', lambda: {})()

def create_results_tab(tab_bar_tag: str, results: Dict[str, Any]):
    """ê²°ê³¼ë¥¼ ìƒˆ íƒ­ì— í‘œì‹œ - SAS Output ìŠ¤íƒ€ì¼"""
    global _viz_tab_counter
    
    if not dpg.does_item_exist(tab_bar_tag):
        return
    
    _viz_tab_counter += 1
    tab_name = f"{results['method']}_{_viz_tab_counter}"
    
    with dpg.tab(label=tab_name, parent=tab_bar_tag, closable=True, tag=f"tab_{tab_name}"):
        with dpg.child_window(border=False, tag=f"scroll_{tab_name}"):
            # ìš”ì•½ ì •ë³´
            _create_summary_info(results)
            dpg.add_separator()
            
            # í†µê³„ í…Œì´ë¸”
            _create_statistical_tables(results)
            dpg.add_separator()
            
            # ì‹œê°í™”
            viz_parent_tag = f"viz_{tab_name}"
            _create_visualizations(viz_parent_tag, results)
            
            # Export ë²„íŠ¼
            dpg.add_separator()
            with dpg.group(horizontal=True):
                dpg.add_button(label="Export to Excel", callback=lambda: _export_results(results, 'excel'))
                dpg.add_button(label="Export to HTML", callback=lambda: _export_results(results, 'html'))

def _create_summary_info(results: Dict[str, Any]):
    """ìš”ì•½ ì •ë³´ ìƒì„±"""
    dpg.add_text(f"Method: {results['method']}", color=(255, 255, 0))
    dpg.add_text(f"DataFrame: {results['df_name']}")
    dpg.add_text(f"Variables: {', '.join(results['variables'])}")
    
    if 'n_clusters' in results:
        dpg.add_text(f"Number of clusters: {results['n_clusters']}")
    if 'n_factors' in results:
        dpg.add_text(f"Number of factors: {results['n_factors']}")
    if 'n_components' in results:
        dpg.add_text(f"Number of components: {results['n_components']}")
    if 'silhouette_avg' in results:
        dpg.add_text(f"Silhouette Score: {results['silhouette_avg']:.3f}")
    if 'explained_variance_ratio' in results:
        total_var = np.sum(results['explained_variance_ratio']) * 100
        dpg.add_text(f"Total Explained Variance: {total_var:.1f}%")
    if 'n_noise' in results:
        dpg.add_text(f"Noise points: {results['n_noise']}")

def _create_statistical_tables(results: Dict[str, Any]):
    """í†µê³„ í…Œì´ë¸” ìƒì„±"""
    method = results['method']
    
    if method in ["K-Means", "Hierarchical", "DBSCAN"]:
        _create_clustering_tables(results)
    elif method == "Factor Analysis":
        _create_factor_tables(results)
    elif method == "PCA":
        _create_pca_tables(results)
    elif method in ["Pearson", "Spearman", "Kendall"]:
        _create_correlation_tables(results)
    elif method in ["Linear", "Logistic"]:
        _create_regression_tables(results)
    elif method == "ANOVA":
        _create_anova_tables(results)
    elif method == "Time Series":
        _create_time_series_tables(results)

def _create_clustering_tables(results: Dict[str, Any]):
    """êµ°ì§‘ë¶„ì„ í…Œì´ë¸”"""
    # í´ëŸ¬ìŠ¤í„° ìš”ì•½
    dpg.add_text("Cluster Summary", color=(255, 255, 0))
    unique_labels, counts = np.unique(results['labels'], return_counts=True)
    percentages = (counts / len(results['labels'])) * 100
    
    summary_data = []
    for label, count, pct in zip(unique_labels, counts, percentages):
        cluster_name = f"Cluster {label}" if label != -1 else "Noise"
        summary_data.append({"Cluster": cluster_name, "Frequency": count, "Percentage": f"{pct:.1f}%"})
    
    _create_table_with_ai("Cluster Summary", pd.DataFrame(summary_data), 
                         f"{results['method']} Cluster Distribution")
    
    # í’ˆì§ˆ ì§€í‘œ
    if 'silhouette_avg' in results or 'inertia' in results:
        dpg.add_text("Quality Metrics", color=(255, 255, 0))
        quality_data = []
        if 'silhouette_avg' in results:
            quality_data.append({
                "Metric": "Silhouette Score", 
                "Value": f"{results['silhouette_avg']:.4f}",
                "Interpretation": _interpret_silhouette(results['silhouette_avg'])
            })
        if 'inertia' in results:
            quality_data.append({
                "Metric": "WCSS", 
                "Value": f"{results['inertia']:.2f}",
                "Interpretation": "Lower is better"
            })
        _create_table_with_ai("Quality Metrics", pd.DataFrame(quality_data),
                             f"{results['method']} Quality Assessment")

def _create_regression_tables(results: Dict[str, Any]):
    """íšŒê·€ë¶„ì„ í…Œì´ë¸” (statsmodels ê²°ê³¼ í¬í•¨)"""
    method = results['method']
    
    # statsmodels summaryê°€ ìˆëŠ” ê²½ìš° ì „ì²´ ìš”ì•½ í‘œì‹œ
    if 'model_summary_text' in results:
        dpg.add_text("Model Summary (statsmodels)", color=(255, 255, 0))
        
        # ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ê³ ì •í­ í°íŠ¸ë¡œ í‘œì‹œ
        summary_lines = results['model_summary_text'].split('\n')
        for line in summary_lines[:50]:  # ì²˜ìŒ 50ì¤„ë§Œ í‘œì‹œ
            dpg.add_text(line, bullet=False)
        
        # AI ë¶„ì„ ë²„íŠ¼ ì¶”ê°€
        ai_button_tag = dpg.generate_uuid()
        summary_text_for_ai = results['model_summary_text']
        
        def analyze_summary():
            import ollama_analyzer
            try:
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
                prompt = f"""ë‹¤ìŒì€ {method} Regression ë¶„ì„ì˜ statsmodels ìš”ì•½ì…ë‹ˆë‹¤:

{summary_text_for_ai}

ì´ íšŒê·€ë¶„ì„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ë‹¤ìŒ ì‚¬í•­ì„ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ì í•©ë„ (R-squared, F-statistic)
2. ìœ ì˜ë¯¸í•œ ë³€ìˆ˜ë“¤ê³¼ ê·¸ ì˜í–¥ë ¥
3. ëª¨ë¸ ì§„ë‹¨ ê²°ê³¼ (ì”ì°¨, ë‹¤ì¤‘ê³µì„ ì„± ë“±)
4. ì‹¤ë¬´ì  ì‹œì‚¬ì ê³¼ ì£¼ì˜ì‚¬í•­
"""
                
                # AI ë¡œê·¸ì— ìŠ¤íŠ¸ë¦¬ë°
                if 'add_ai_log' in _module_main_callbacks:
                    first_chunk = True
                    for chunk in ollama_analyzer.analyze_text_with_ollama(prompt, f"{method} Regression Summary"):
                        if first_chunk:
                            _module_main_callbacks['add_ai_log'](chunk, f"{method} Regression Summary", mode="stream_start_entry")
                            first_chunk = False
                        else:
                            _module_main_callbacks['add_ai_log'](chunk, "", mode="stream_chunk_append")
                    
                    _module_main_callbacks['add_ai_log']("\n(ë¶„ì„ ì™„ë£Œ)", "", mode="stream_chunk_append")
                
            except Exception as e:
                if 'add_ai_log' in _module_main_callbacks:
                    _module_main_callbacks['add_ai_log'](f"Error: {str(e)}", f"{method} Regression Summary", mode="new_log_entry")
        
        dpg.add_button(label="ğŸ’¡ Analyze Summary with AI", tag=ai_button_tag,
                      callback=analyze_summary)
        dpg.add_separator()
    
    # ê³„ìˆ˜ í…Œì´ë¸”
    dpg.add_text("Model Coefficients", color=(255, 255, 0))
    coef_df = results['coefficients'].copy()
    _create_table_with_ai("Model Coefficients", coef_df,
                         f"{method} Regression Coefficients")
    
    # ì„±ëŠ¥ ì§€í‘œ
    if 'diagnostics' in results:
        dpg.add_text("Diagnostic Tests", color=(255, 255, 0))
        diag = results['diagnostics']
        
        # VIF (ë‹¤ì¤‘ê³µì„ ì„±)
        if 'vif' in diag:
            _create_table_with_ai("Variance Inflation Factors", diag['vif'],
                                "Multicollinearity Assessment")
        
        # Breusch-Pagan (ì´ë¶„ì‚°ì„±)
        if 'breusch_pagan' in diag:
            bp_data = pd.DataFrame([{
                'Test': 'Breusch-Pagan',
                'LM Statistic': f"{diag['breusch_pagan']['lm_statistic']:.4f}",
                'LM p-value': f"{diag['breusch_pagan']['lm_pvalue']:.4f}",
                'F Statistic': f"{diag['breusch_pagan']['f_statistic']:.4f}",
                'F p-value': f"{diag['breusch_pagan']['f_pvalue']:.4f}",
                'Heteroscedasticity': 'Present' if diag['breusch_pagan']['lm_pvalue'] < 0.05 else 'Not detected'
            }])
            _create_table_with_ai("Heteroscedasticity Test", bp_data,
                                "Breusch-Pagan Test Results")
        
        # Durbin-Watson (ìê¸°ìƒê´€)
        if 'durbin_watson' in diag:
            dw_interpretation = "Positive autocorrelation" if diag['durbin_watson'] < 1.5 else \
                               "No autocorrelation" if 1.5 <= diag['durbin_watson'] <= 2.5 else \
                               "Negative autocorrelation"
            dw_data = pd.DataFrame([{
                'Test': 'Durbin-Watson',
                'Statistic': f"{diag['durbin_watson']:.4f}",
                'Interpretation': dw_interpretation
            }])
            _create_table_with_ai("Autocorrelation Test", dw_data,
                                "Durbin-Watson Test Results")

def _create_anova_tables(results: Dict[str, Any]):
    """ANOVA í…Œì´ë¸”"""
    dpg.add_text(f"{results['anova_type']} Results", color=(255, 255, 0))
    
    # ANOVA í…Œì´ë¸”
    anova_table = results['anova_table'].copy()
    anova_table = anova_table.round(4)
    _create_table_with_ai("ANOVA Table", anova_table, results['anova_type'])
    
    # Post-hoc ê²€ì • ê²°ê³¼
    if 'post_hoc' in results and results['post_hoc']:
        if 'tukey_hsd' in results['post_hoc']:
            dpg.add_text("Tukey HSD Post-hoc Test", color=(255, 255, 0))
            tukey = results['post_hoc']['tukey_hsd']
            # Tukey ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            tukey_df = pd.DataFrame(data=tukey._results_table.data[1:], 
                                  columns=tukey._results_table.data[0])
            _create_table_with_ai("Tukey HSD Results", tukey_df, 
                                "Multiple Comparisons")

def _create_time_series_tables(results: Dict[str, Any]):
    """ì‹œê³„ì—´ ë¶„ì„ í…Œì´ë¸”"""
    dpg.add_text("Time Series Analysis Results", color=(255, 255, 0))
    
    # ê¸°ë³¸ í†µê³„
    stats_df = pd.DataFrame([results['statistics']])
    _create_table_with_ai("Time Series Statistics", stats_df,
                         "Basic Time Series Properties")
    
    # ADF ê²€ì • ê²°ê³¼
    adf = results['adf_test']
    adf_df = pd.DataFrame([{
        'ADF Statistic': f"{adf['adf_statistic']:.4f}",
        'p-value': f"{adf['p_value']:.4f}",
        'Used Lag': adf['used_lag'],
        'Critical Value (1%)': f"{adf['critical_values']['1%']:.4f}",
        'Critical Value (5%)': f"{adf['critical_values']['5%']:.4f}",
        'Critical Value (10%)': f"{adf['critical_values']['10%']:.4f}",
        'Stationary': 'Yes' if adf['is_stationary'] else 'No'
    }])
    _create_table_with_ai("Augmented Dickey-Fuller Test", adf_df,
                         "Stationarity Test Results")

def _create_table_with_ai(title: str, df: pd.DataFrame, context: str = ""):
    """í…Œì´ë¸” ìƒì„± with AI ë¶„ì„ ë²„íŠ¼"""
    table_tag = dpg.generate_uuid()
    with dpg.table(header_row=True, resizable=True, tag=table_tag,
                   borders_outerH=True, borders_innerV=True, borders_innerH=True, borders_outerV=True):
        if _util_funcs and 'create_table_with_data' in _util_funcs:
            _util_funcs['create_table_with_data'](table_tag, df)
    
    # AI ë¶„ì„ ë²„íŠ¼ ì¶”ê°€
    if df is not None and not df.empty:
        ai_button_tag = dpg.generate_uuid()
        
        def analyze_table():
            # DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            table_text = df.to_string()
            
            import ollama_analyzer
            try:
                # í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
                prompt = f"""ë‹¤ìŒì€ '{title}' í…Œì´ë¸”ì…ë‹ˆë‹¤:

{table_text}

Context: {context}

ì´ í…Œì´ë¸”ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³  í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ë°œê²¬ì‚¬í•­
2. í†µê³„ì  ì˜ë¯¸
3. ì‹¤ë¬´ì  ì‹œì‚¬ì 
"""
                
                # AI ë¡œê·¸ì— ìŠ¤íŠ¸ë¦¬ë°
                if 'add_ai_log' in _module_main_callbacks:
                    first_chunk = True
                    for chunk in ollama_analyzer.analyze_text_with_ollama(prompt, f"Table: {title}"):
                        if first_chunk:
                            _module_main_callbacks['add_ai_log'](chunk, f"Table: {title}", mode="stream_start_entry")
                            first_chunk = False
                        else:
                            _module_main_callbacks['add_ai_log'](chunk, "", mode="stream_chunk_append")
                    
                    _module_main_callbacks['add_ai_log']("\n(ë¶„ì„ ì™„ë£Œ)", "", mode="stream_chunk_append")
                
            except Exception as e:
                if 'add_ai_log' in _module_main_callbacks:
                    _module_main_callbacks['add_ai_log'](f"Error: {str(e)}", f"Table: {title}", mode="new_log_entry")
        
        dpg.add_button(label=f"ğŸ’¡ Analyze {title}", tag=ai_button_tag,
                      callback=analyze_table, small=True)
    
    dpg.add_spacer(height=10)

def _create_factor_tables(results: Dict[str, Any]):
    """ìš”ì¸ë¶„ì„ í…Œì´ë¸”"""
    # ë¡œë”© í–‰ë ¬
    dpg.add_text("Factor Loadings", color=(255, 255, 0))
    loadings_df = pd.DataFrame(
        results['loadings'], 
        index=results['variables'],
        columns=[f'Factor {i+1}' for i in range(results['n_factors'])]
    ).reset_index().rename(columns={'index': 'Variable'})
    _create_table_with_ai("Factor Loadings", loadings_df, "Factor Analysis Loadings Matrix")
    
    # ì„¤ëª… ë¶„ì‚°
    dpg.add_text("Variance Explained", color=(255, 255, 0))
    variance_data = []
    cumulative = 0
    for i in range(results['n_factors']):
        var_pct = results['explained_variance_ratio'][i] * 100
        cumulative += var_pct
        variance_data.append({
            "Factor": f"Factor {i+1}",
            "Variance %": f"{var_pct:.2f}%",
            "Cumulative %": f"{cumulative:.2f}%"
        })
    _create_table_with_ai("Variance Explained", pd.DataFrame(variance_data),
                         "Factor Analysis Variance Decomposition")

def _create_pca_tables(results: Dict[str, Any]):
    """PCA í…Œì´ë¸”"""
    # ì„¤ëª… ë¶„ì‚°
    dpg.add_text("Principal Components", color=(255, 255, 0))
    variance_data = []
    cumulative = 0
    for i in range(results['n_components']):
        var_pct = results['explained_variance_ratio'][i] * 100
        cumulative += var_pct
        variance_data.append({
            "Component": f"PC{i+1}",
            "Eigenvalue": f"{results['explained_variance'][i]:.4f}",
            "Variance %": f"{var_pct:.2f}%",
            "Cumulative %": f"{cumulative:.2f}%"
        })
    _create_table_with_ai("Principal Components", pd.DataFrame(variance_data),
                         "PCA Variance Decomposition")

def _create_correlation_tables(results: Dict[str, Any]):
    """ìƒê´€ë¶„ì„ í…Œì´ë¸”"""
    dpg.add_text("Correlation Matrix", color=(255, 255, 0))
    
    # ìƒê´€ê³„ìˆ˜ í–‰ë ¬
    corr_matrix = results['correlation_matrix']
    corr_display = corr_matrix.reset_index()
    _create_table_with_ai("Correlation Matrix", corr_display,
                         f"{results['method']} Correlation Analysis")
    
    # p-value í–‰ë ¬
    if 'p_value_matrix' in results:
        dpg.add_text("P-values", color=(255, 255, 0))
        pval_display = results['p_value_matrix'].reset_index()
        _create_table_with_ai("P-value Matrix", pval_display,
                            "Statistical Significance of Correlations")

def _create_visualizations(parent_tag: str, results: Dict[str, Any]):
    """ì‹œê°í™” ìƒì„±"""
    method = results['method']
    plot_func = _util_funcs.get('plot_to_dpg_texture')
    
    if method == "K-Means":
        _create_kmeans_plots(parent_tag, results, plot_func)
    elif method == "Hierarchical":
        _create_hierarchical_plots(parent_tag, results, plot_func)
    elif method == "DBSCAN":
        _create_dbscan_plots(parent_tag, results, plot_func)
    elif method == "Factor Analysis":
        _create_factor_plots(parent_tag, results, plot_func)
    elif method == "PCA":
        _create_pca_plots(parent_tag, results, plot_func)
    elif method in ["Pearson", "Spearman", "Kendall"]:
        _create_correlation_plots(parent_tag, results, plot_func)
    elif method in ["Linear", "Logistic"]:
        _create_regression_plots(parent_tag, results, plot_func)
    elif method == "ANOVA":
        _create_anova_plots(parent_tag, results, plot_func)
    elif method == "Time Series":
        _create_time_series_plots(parent_tag, results, plot_func)

def _create_anova_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """ANOVA ì‹œê°í™”"""
    # ì”ì°¨ í”Œë¡¯
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    residuals = results['residuals']
    fitted = results['fitted_values']
    
    # Residuals vs Fitted
    axes[0, 0].scatter(fitted, residuals, alpha=0.6)
    axes[0, 0].axhline(y=0, color='r', linestyle='--')
    axes[0, 0].set_xlabel('Fitted Values')
    axes[0, 0].set_ylabel('Residuals')
    axes[0, 0].set_title('Residuals vs Fitted')
    
    # Q-Q plot
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[0, 1])
    axes[0, 1].set_title('Normal Q-Q Plot')
    
    # Histogram of residuals
    axes[1, 0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)
    axes[1, 0].set_xlabel('Residuals')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Histogram of Residuals')
    
    # Scale-Location plot
    standardized_residuals = residuals / np.std(residuals)
    axes[1, 1].scatter(fitted, np.sqrt(np.abs(standardized_residuals)), alpha=0.6)
    axes[1, 1].set_xlabel('Fitted Values')
    axes[1, 1].set_ylabel('âˆš|Standardized Residuals|')
    axes[1, 1].set_title('Scale-Location Plot')
    
    plt.tight_layout()
    _add_plot(fig, plot_func, parent_tag, "ANOVA Diagnostic Plots")

def _create_time_series_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """ì‹œê³„ì—´ ë¶„ì„ ì‹œê°í™”"""
    ts_data = results['time_series']
    
    # ì‹œê³„ì—´ í”Œë¡¯
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(ts_data.index, ts_data.values)
    ax.set_xlabel('Time')
    ax.set_ylabel(results['variable'])
    ax.set_title('Time Series Plot')
    ax.grid(True, alpha=0.3)
    _add_plot(fig, plot_func, parent_tag, "Time Series Plot")
    
    # ACF/PACF í”Œë¡¯
    from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
    
    fig, axes = plt.subplots(2, 1, figsize=(10, 8))
    plot_acf(ts_data, lags=min(40, len(ts_data)//4), ax=axes[0])
    plot_pacf(ts_data, lags=min(40, len(ts_data)//4), ax=axes[1])
    plt.tight_layout()
    _add_plot(fig, plot_func, parent_tag, "ACF/PACF Plots")
    
    # ê³„ì ˆì„± ë¶„í•´
    if results.get('decomposition'):
        decomp = results['decomposition']
        fig, axes = plt.subplots(4, 1, figsize=(10, 10))
        
        ts_data.plot(ax=axes[0])
        axes[0].set_title('Original Time Series')
        
        decomp.trend.plot(ax=axes[1])
        axes[1].set_title('Trend Component')
        
        decomp.seasonal.plot(ax=axes[2])
        axes[2].set_title('Seasonal Component')
        
        decomp.resid.plot(ax=axes[3])
        axes[3].set_title('Residual Component')
        
        plt.tight_layout()
        _add_plot(fig, plot_func, parent_tag, "Seasonal Decomposition")

# ê¸°ì¡´ í”Œë¡¯ í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€...
def _create_kmeans_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """K-Means ì‹œê°í™”"""
    # Elbow Plot
    if 'elbow_data' in results:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.plot(results['elbow_data']['K'], results['elbow_data']['inertia'], 'bo-')
        ax.set_xlabel('Number of clusters (k)')
        ax.set_ylabel('Inertia')
        ax.set_title('Elbow Method')
        ax.grid(True, alpha=0.3)
        _add_plot(fig, plot_func, parent_tag, "Elbow Plot")
    
    # Cluster Distribution
    fig, ax = plt.subplots(figsize=(8, 5))
    unique_labels, counts = np.unique(results['labels'], return_counts=True)
    ax.bar(unique_labels, counts, color=plt.cm.viridis(np.linspace(0, 1, len(unique_labels))))
    ax.set_xlabel('Cluster')
    ax.set_ylabel('Count')
    ax.set_title('Cluster Distribution')
    _add_plot(fig, plot_func, parent_tag, "Cluster Distribution")

def _create_hierarchical_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """ê³„ì¸µì  êµ°ì§‘ë¶„ì„ ì‹œê°í™”"""
    # Dendrogram
    if 'linkage_matrix' in results:
        fig, ax = plt.subplots(figsize=(10, 6))
        dendrogram(results['linkage_matrix'], ax=ax)
        ax.set_title('Dendrogram')
        _add_plot(fig, plot_func, parent_tag, "Dendrogram")

def _create_dbscan_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """DBSCAN ì‹œê°í™”"""
    # Cluster Distribution
    fig, ax = plt.subplots(figsize=(8, 5))
    unique_labels, counts = np.unique(results['labels'], return_counts=True)
    colors = ['red' if label == -1 else plt.cm.viridis(label/max(1, results['n_clusters'])) 
              for label in unique_labels]
    ax.bar(range(len(unique_labels)), counts, color=colors)
    ax.set_xlabel('Cluster')
    ax.set_ylabel('Count')
    ax.set_title('DBSCAN Results')
    _add_plot(fig, plot_func, parent_tag, "DBSCAN Cluster Distribution")

def _create_factor_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """ìš”ì¸ë¶„ì„ ì‹œê°í™”"""
    # Scree Plot
    if 'eigenvalues' in results:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.plot(range(1, len(results['eigenvalues']) + 1), results['eigenvalues'], 'bo-')
        ax.axhline(y=1, color='red', linestyle='--', label='Kaiser Criterion')
        ax.set_xlabel('Factor Number')
        ax.set_ylabel('Eigenvalue')
        ax.set_title('Scree Plot')
        ax.legend()
        _add_plot(fig, plot_func, parent_tag, "Scree Plot")
    
    # Loadings Heatmap
    fig, ax = plt.subplots(figsize=(8, 6))
    loadings_df = pd.DataFrame(
        results['loadings'], 
        index=results['variables'],
        columns=[f'Factor {i+1}' for i in range(results['n_factors'])]
    )
    sns.heatmap(loadings_df, annot=True, cmap='RdBu_r', center=0, ax=ax)
    ax.set_title('Factor Loadings')
    _add_plot(fig, plot_func, parent_tag, "Factor Loadings Heatmap")

def _create_pca_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """PCA ì‹œê°í™”"""
    # Scree Plot
    if 'eigenvalues' in results:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.plot(range(1, len(results['eigenvalues']) + 1), results['eigenvalues'], 'bo-')
        ax.axhline(y=1, color='red', linestyle='--', label='Kaiser Criterion')
        ax.set_xlabel('Component Number')
        ax.set_ylabel('Eigenvalue')
        ax.set_title('Scree Plot')
        ax.legend()
        _add_plot(fig, plot_func, parent_tag, "Scree Plot")
    
    # Scores Plot
    if results['n_components'] >= 2:
        fig, ax = plt.subplots(figsize=(8, 6))
        scores = results['component_scores']
        ax.scatter(scores[:, 0], scores[:, 1], alpha=0.6)
        pc1_var = results['explained_variance_ratio'][0] * 100
        pc2_var = results['explained_variance_ratio'][1] * 100
        ax.set_xlabel(f'PC1 ({pc1_var:.1f}%)')
        ax.set_ylabel(f'PC2 ({pc2_var:.1f}%)')
        ax.set_title('PCA Scores Plot')
        _add_plot(fig, plot_func, parent_tag, "PCA Scores Plot")

def _create_correlation_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """ìƒê´€ë¶„ì„ ì‹œê°í™”"""
    # Correlation Heatmap
    fig, ax = plt.subplots(figsize=(8, 6))
    corr_matrix = results['correlation_matrix']
    
    # Significance mask
    if 'p_value_matrix' in results:
        alpha = results.get('significance_level', 0.05)
        mask = results['p_value_matrix'] > alpha
        
        # Annotate with stars for significance
        annot_data = corr_matrix.copy()
        for i in range(len(corr_matrix)):
            for j in range(len(corr_matrix.columns)):
                if i != j:
                    p_val = results['p_value_matrix'].iloc[i, j]
                    if p_val < 0.001:
                        annot_data.iloc[i, j] = f"{corr_matrix.iloc[i, j]:.2f}***"
                    elif p_val < 0.01:
                        annot_data.iloc[i, j] = f"{corr_matrix.iloc[i, j]:.2f}**"
                    elif p_val < 0.05:
                        annot_data.iloc[i, j] = f"{corr_matrix.iloc[i, j]:.2f}*"
                    else:
                        annot_data.iloc[i, j] = f"{corr_matrix.iloc[i, j]:.2f}"
                else:
                    annot_data.iloc[i, j] = "1.00"
        
        sns.heatmap(corr_matrix, annot=annot_data, fmt='', cmap='RdBu_r', center=0, 
                    ax=ax, square=True)
    else:
        sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, ax=ax, 
                    square=True, fmt='.3f')
    
    ax.set_title(f'{results["method"]} Correlation Matrix')
    _add_plot(fig, plot_func, parent_tag, f"{results['method']} Correlation Heatmap")

def _create_regression_plots(parent_tag: str, results: Dict[str, Any], plot_func):
    """íšŒê·€ë¶„ì„ ì‹œê°í™”"""
    method = results['method']
    
    if method == "Linear":
        # ì§„ë‹¨ í”Œë¡¯ (4ê°œ íŒ¨ë„)
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. Residuals vs Fitted
        axes[0, 0].scatter(results['predictions_test'], results['residuals_test'], alpha=0.6)
        axes[0, 0].axhline(y=0, color='r', linestyle='--')
        axes[0, 0].set_xlabel('Fitted Values')
        axes[0, 0].set_ylabel('Residuals')
        axes[0, 0].set_title('Residuals vs Fitted')
        
        # 2. Q-Q plot
        from scipy import stats
        stats.probplot(results['residuals_test'], dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Normal Q-Q Plot')
        
        # 3. Scale-Location
        standardized_residuals = results['residuals_test'] / np.std(results['residuals_test'])
        axes[1, 0].scatter(results['predictions_test'], np.sqrt(np.abs(standardized_residuals)), alpha=0.6)
        axes[1, 0].set_xlabel('Fitted Values')
        axes[1, 0].set_ylabel('âˆš|Standardized Residuals|')
        axes[1, 0].set_title('Scale-Location')
        
        # 4. Residuals vs Leverage
        if 'diagnostics' in results and 'leverage' in results['diagnostics']:
            leverage = results['diagnostics']['leverage'][:len(results['residuals_test'])]
            axes[1, 1].scatter(leverage, standardized_residuals[:len(leverage)], alpha=0.6)
            axes[1, 1].set_xlabel('Leverage')
            axes[1, 1].set_ylabel('Standardized Residuals')
            axes[1, 1].set_title('Residuals vs Leverage')
        else:
            # Actual vs Predicted as alternative
            axes[1, 1].scatter(results['y_test'], results['predictions_test'], alpha=0.6)
            axes[1, 1].plot([results['y_test'].min(), results['y_test'].max()], 
                           [results['y_test'].min(), results['y_test'].max()], 'r--', lw=2)
            axes[1, 1].set_xlabel('Actual Values')
            axes[1, 1].set_ylabel('Predicted Values')
            axes[1, 1].set_title('Actual vs Predicted')
        
        plt.tight_layout()
        _add_plot(fig, plot_func, parent_tag, "Regression Diagnostic Plots")
        
    elif method == "Logistic":
        # ROC Curve
        from sklearn.metrics import roc_curve, auc
        
        if 'prediction_probabilities_test' in results:
            fig, ax = plt.subplots(figsize=(8, 6))
            
            fpr, tpr, _ = roc_curve(results['y_test'], results['prediction_probabilities_test'])
            roc_auc = auc(fpr, tpr)
            
            ax.plot(fpr, tpr, color='darkorange', lw=2, 
                   label=f'ROC curve (AUC = {roc_auc:.2f})')
            ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title('Receiver Operating Characteristic')
            ax.legend(loc="lower right")
            _add_plot(fig, plot_func, parent_tag, "ROC Curve")
        
        # Confusion Matrix
        fig, ax = plt.subplots(figsize=(6, 5))
        cm = results['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        ax.set_title('Confusion Matrix')
        _add_plot(fig, plot_func, parent_tag, "Confusion Matrix")
    
    # Feature Importance (ê³„ìˆ˜ í”Œë¡¯)
    fig, ax = plt.subplots(figsize=(8, 6))
    coef_df = results['coefficients'].copy()
    
    # ìƒìˆ˜í•­ ì œì™¸
    if 'const' in coef_df['Feature'].values:
        coef_df = coef_df[coef_df['Feature'] != 'const']
    
    coef_df['Abs_Coefficient'] = np.abs(coef_df['Coefficient'])
    coef_df = coef_df.sort_values('Abs_Coefficient')
    
    colors = ['red' if x < 0 else 'blue' for x in coef_df['Coefficient']]
    ax.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, alpha=0.7)
    ax.set_xlabel('Coefficient Value')
    ax.set_title('Feature Coefficients')
    ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)
    _add_plot(fig, plot_func, parent_tag, "Feature Coefficients")

def _add_plot(fig, plot_func, parent_tag, chart_name="Chart"):
    """í”Œë¡¯ì„ DPGì— ì¶”ê°€í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (AI ë¶„ì„ ë²„íŠ¼ í¬í•¨)"""
    tex_tag, w, h, img_bytes = plot_func(fig)
    if tex_tag:
        _texture_tags.append(tex_tag)
        dpg.add_image(tex_tag, parent=parent_tag, width=w, height=h)
        
        # AI ë¶„ì„ ë²„íŠ¼ ì¶”ê°€
        if img_bytes:
            ai_button_tag = dpg.generate_uuid()
            action_callback = functools.partial(
                utils.confirm_and_run_ai_analysis,
                img_bytes, chart_name, ai_button_tag, _module_main_callbacks
            )
            dpg.add_button(label=f"ğŸ’¡ Analyze {chart_name}", tag=ai_button_tag,
                          callback=lambda s, a, u: action_callback())
    
    plt.close(fig)
    dpg.add_separator(parent=parent_tag)

def _interpret_silhouette(score: float) -> str:
    """ì‹¤ë£¨ì—£ ì ìˆ˜ í•´ì„"""
    if score > 0.7:
        return "Strong"
    elif score > 0.5:
        return "Reasonable"
    elif score > 0.25:
        return "Weak"
    else:
        return "Poor"

def _export_results(results: Dict[str, Any], format: str):
    """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
    import step_10_analysis as analysis
    analysis.export_results(results, format)

def reset_state():
    """ìƒíƒœ ì´ˆê¸°í™”"""
    global _viz_tab_counter, _texture_tags
    _viz_tab_counter = 0
    
    for tag in _texture_tags:
        if dpg.does_item_exist(tag):
            dpg.delete_item(tag)
    _texture_tags.clear()