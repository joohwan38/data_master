# step_11_ml_modeling.py - 2-Track í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• êµ¬í˜„

"""
Step 11 ML Modeling & AI í†µí•© ëª¨ë“ˆ (í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•)

ì£¼ìš” ê¸°ëŠ¥:
- 2-Track í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•:
  - Track 1: ìë™í™”ëœ ëª¨ë¸ íƒìƒ‰ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í¬í•¨) ë° ë¦¬ë”ë³´ë“œ
  - Track 2: ì‹¬ì¸µ ë¶„ì„, ì‚¬ìš©ì ì •ì˜ íŠœë‹ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
- ëª¨ë¸ ì¸ì‚¬ì´íŠ¸ ë©: SHAP Beeswarm, Dependence Plot ë“± ë‹¤ì–‘í•œ XAI ì‹œê°í™”
- ì•™ìƒë¸”: ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ìƒìœ„ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ ê·¹ëŒ€í™”
- ëª¨ë¸ ì„œë¹™: í•™ìŠµëœ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì„ ë‚´ë³´ë‚´ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡  ìˆ˜í–‰
- ê³ ê¸‰ í›ˆë ¨ ì˜µì…˜: CV í´ë“œ, ìµœì í™” ì§€í‘œ ë“± ì„¸ë¶€ ì œì–´
- ë”¥ëŸ¬ë‹ í™•ì¥ ê¸°ë°˜: MLP ëª¨ë¸ ì¶”ê°€ë¥¼ ìœ„í•œ UI ë° ì½”ë“œ êµ¬ì¡° í¬í•¨
"""

import dearpygui.dearpygui as dpg
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
import traceback
import datetime
import uuid
import threading
import queue
import time
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import joblib
import os

# Scikit-learn ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,
                           roc_curve, roc_auc_score,auc, mean_squared_error, r2_score, mean_absolute_error, make_scorer,
                           cohen_kappa_score, matthews_corrcoef)

from sklearn.ensemble import VotingClassifier, VotingRegressor

# ëª¨ë¸ ì•Œê³ ë¦¬ì¦˜
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor
from lightgbm import LGBMClassifier, LGBMRegressor

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì„¤ëª… ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# ì‚¬ìš©ì ì°¸ê³ : ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ tensorflow ë˜ëŠ” pytorch ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
# try:
#     import tensorflow as tf
#     from tensorflow.keras.models import Sequential
#     from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
#     TENSORFLOW_AVAILABLE = True
# except ImportError:
#     TENSORFLOW_AVAILABLE = False
#     print("Warning: TensorFlow ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
TENSORFLOW_AVAILABLE = False # ìš°ì„  ë¹„í™œì„±í™”


warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# --- DPG Tags ---
TAG_S11_GROUP = "step11_ml_modeling_group"
TAG_S11_MAIN_TAB_BAR = "step11_main_tab_bar"
TAG_S11_MODELING_TAB = "step11_modeling_tab"
TAG_S11_INFERENCE_TAB = "step11_inference_tab"

# Modeling Tab UI
TAG_S11_DF_SELECTOR = "step11_df_selector"
TAG_S11_TARGET_SELECTOR = "step11_target_selector"
TAG_S11_TASK_TYPE_TEXT = "s11_detected_task_text"
TAG_S11_RUN_AUTO_DISCOVERY_BUTTON = "step11_run_auto_discovery_button"
TAG_S11_LEADERBOARD_TABLE = "step11_leaderboard_table"
TAG_S11_LEADERBOARD_GROUP = "step11_leaderboard_group"
TAG_S11_ENSEMBLE_BUTTON = "step11_ensemble_button"
TAG_S11_DEEP_DIVE_GROUP = "step11_deep_dive_group"
TAG_S11_LOG_TEXT = "step11_log_text"
TAG_S11_LOG_WINDOW = "step11_log_window"
TAG_S11_PROGRESS_BAR = "step11_progress_bar"

# --- ML ì•Œê³ ë¦¬ì¦˜ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ ë²”ìœ„ ì •ì˜ ---
# RandomizedSearchCVë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ë¶„í¬ ì •ì˜
PARAM_DISTRIBUTIONS = {
    "Classification": {
        "Random Forest": {
            "model_class": RandomForestClassifier,
            "params": {
                'classifier__n_estimators': [100, 200, 300],
                'classifier__max_depth': [10, 20, 30, None],
                'classifier__min_samples_split': [2, 5, 10],
                'classifier__min_samples_leaf': [1, 2, 4]
            }
        },
        "XGBoost": {
            "model_class": XGBClassifier,
            "params": {
                'classifier__n_estimators': [100, 200, 300],
                'classifier__learning_rate': [0.01, 0.1, 0.2],
                'classifier__max_depth': [3, 5, 7],
                'classifier__subsample': [0.7, 0.8, 0.9]
            }
        },
        "LightGBM": {
            "model_class": LGBMClassifier,
            "params": {
                'classifier__n_estimators': [100, 200, 300],
                'classifier__learning_rate': [0.01, 0.1, 0.2],
                'classifier__num_leaves': [20, 31, 40],
                'classifier__max_depth': [-1, 10, 20]
            }
        }
    },
    "Regression": {
        "Random Forest": {
            "model_class": RandomForestRegressor,
            "params": {
                'regressor__n_estimators': [100, 200, 300],
                'regressor__max_depth': [10, 20, 30, None],
                'regressor__min_samples_split': [2, 5, 10],
                'regressor__min_samples_leaf': [1, 2, 4]
            }
        },
        "XGBoost": {
            "model_class": XGBRegressor,
            "params": {
                'regressor__n_estimators': [100, 200, 300],
                'regressor__learning_rate': [0.01, 0.1, 0.2],
                'regressor__max_depth': [3, 5, 7],
                'regressor__subsample': [0.7, 0.8, 0.9]
            }
        },
        "LightGBM": {
            "model_class": LGBMRegressor,
            "params": {
                'regressor__n_estimators': [100, 200, 300],
                'regressor__learning_rate': [0.01, 0.1, 0.2],
                'regressor__num_leaves': [20, 31, 40],
                'regressor__max_depth': [-1, 10, 20]
            }
        }
    }
}
# ê¸°ë³¸ ëª¨ë¸ (íŠœë‹ ì—†ì´ ë¹ ë¥´ê²Œ ì‹¤í–‰)
BASE_MODELS = {
    "Classification": {"Logistic Regression": LogisticRegression(max_iter=1000, random_state=42)},
    "Regression": {"Linear Regression": LinearRegression()}
}


# --- Module State ---
_module_main_callbacks: Optional[Dict] = None
_util_funcs: Optional[Dict[str, Any]] = None
_texture_tags: List[str] = []
_results_queue = queue.Queue()
_worker_thread: Optional[threading.Thread] = None
_leaderboard_results: List[Dict] = []
_current_deep_dive_model: Optional[Dict] = None

def _calculate_all_metrics(pipeline, X_test, y_test, task_type):
    """ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    y_pred = pipeline.predict(X_test)
    # metrics ë”•ì…”ë„ˆë¦¬ë¥¼ ë¹„ì–´ìˆëŠ” ìƒíƒœì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.
    metrics = {}

    if task_type == "Classification":
        y_proba = pipeline.predict_proba(X_test)[:, 1]
        metrics["Accuracy"] = accuracy_score(y_test, y_pred)
        metrics["AUC"] = roc_auc_score(y_test, y_proba)
        metrics["Recall"] = recall_score(y_test, y_pred, average='macro', zero_division=0)
        metrics["Prec."] = precision_score(y_test, y_pred, average='macro', zero_division=0)
        metrics["F1"] = f1_score(y_test, y_pred, average='macro', zero_division=0)
        metrics["Kappa"] = cohen_kappa_score(y_test, y_pred)
        metrics["MCC"] = matthews_corrcoef(y_test, y_pred)
    else: # Regression
        metrics["MAE"] = mean_absolute_error(y_test, y_pred)
        metrics["MSE"] = mean_squared_error(y_test, y_pred)
        metrics["RMSE"] = np.sqrt(mean_squared_error(y_test, y_pred))
        metrics["R2"] = r2_score(y_test, y_pred)

    return metrics


# --- Data Preparation ---
def _prepare_data_for_modeling(df: pd.DataFrame, target_name: str) -> Tuple:
    """
    ë°ì´í„°ë¥¼ ëª¨ë¸ë§ì— ë§ê²Œ ì¤€ë¹„. Step 1-10ì˜ ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìµœëŒ€í•œ ì¡´ì¤‘.
    ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì¸ì½”ë”©, ë°ì´í„° ë¶„í• ì„ ìˆ˜í–‰. ìŠ¤ì¼€ì¼ë§ì€ íŒŒì´í”„ë¼ì¸ì˜ ì¼ë¶€ë¡œ ë‚¨ê²¨ë‘ .
    """
    _log_message("ë°ì´í„° ì¤€ë¹„ ì‹œì‘: X, y ë¶„ë¦¬...")
    X = df.drop(columns=[target_name])
    y = df[target_name]

    task_type = "Regression"
    le_map = {} # ë¼ë²¨ ì¸ì½”ë”© ì •ë³´ ì €ì¥
    if y.dtype == 'object' or y.nunique() < 25:
        task_type = "Classification"
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        le_map = dict(zip(le.classes_, le.transform(le.classes_)))
        y = pd.Series(y_encoded, index=y.index, name=y.name)
        _log_message(f"íƒ€ê²Ÿ ë³€ìˆ˜ ë¼ë²¨ ì¸ì½”ë”© ì™„ë£Œ. ë§¤í•‘: {le_map}")

    _log_message(f"íƒœìŠ¤í¬ íƒ€ì… ê°ì§€: {task_type}")

    numeric_features = X.select_dtypes(include=np.number).columns.tolist()
    categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()
    _log_message(f"ìˆ«ìí˜• í”¼ì²˜: {len(numeric_features)}ê°œ, ë²”ì£¼í˜• í”¼ì²˜: {len(categorical_features)}ê°œ")

    # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜ (ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ì¸ì½”ë”©)
    # ìŠ¤ì¼€ì¼ë§ì€ ê° ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì— í¬í•¨í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œ ì •ë³´ ìœ ì¶œ ë°©ì§€
    numeric_transformer = SimpleImputer(strategy='median')
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)],
        remainder='passthrough')

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y if task_type == 'Classification' else None)
    _log_message("ë°ì´í„° ë¶„í•  ì™„ë£Œ (í›ˆë ¨ 75% / í…ŒìŠ¤íŠ¸ 25%).")

    return X, y, X_train, X_test, y_train, y_test, preprocessor, task_type, le_map

# --- Background Thread Functions ---
def _run_automated_discovery_thread(df: pd.DataFrame, target_name: str, cv_folds: int, optimization_metric: str):
    print("\n" + "="*60)
    print(">>> STEP 11 DEBUG: Modeling function received the following DataFrame:")
    print(df.info())
    print("="*60 + "\n")
    """[Track 1] ìë™í™”ëœ ëª¨ë¸ íƒìƒ‰ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜í–‰"""
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        _results_queue.put({"type": "progress", "value": 0.05, "log": "ë°ì´í„° ì¤€ë¹„ ì¤‘..."})
        X, y, X_train, X_test, y_train, y_test, preprocessor, task_type, le_map = _prepare_data_for_modeling(df, target_name)

        # 2. íƒìƒ‰í•  ëª¨ë¸ ëª©ë¡ ì •ì˜
        models_to_tune = PARAM_DISTRIBUTIONS.get(task_type, {})
        base_models_to_run = BASE_MODELS.get(task_type, {})
        all_models = list(base_models_to_run.keys()) + list(models_to_tune.keys())
        total_steps = len(all_models)
        
        # Scikit-learnì˜ scoring ì´ë¦„ê³¼ í‘œì‹œ ì´ë¦„ì„ ë§¤í•‘
        metric_name_map = {
            "accuracy": "Accuracy", "f1": "F1", "recall": "Recall",
            "precision": "Prec.", "roc_auc": "AUC",
            "r2": "R2", "neg_mean_absolute_error": "MAE", "neg_mean_squared_error": "MSE"
        }
        # UIì—ì„œ ë°›ì€ ê°’(e.g., 'F1')ì„ scikit-learnì´ ì´í•´í•˜ëŠ” ì´ë¦„(e.g., 'f1')ìœ¼ë¡œ ë³€í™˜
        inverse_metric_name_map = {v: k for k, v in metric_name_map.items()}
        sklearn_metric_name = inverse_metric_name_map.get(optimization_metric, optimization_metric.lower())


        # 3. ëª¨ë¸ í›ˆë ¨ ë° íŠœë‹ ë£¨í”„
        results = []
        for i, model_name in enumerate(all_models):
            progress = 0.1 + (i / total_steps) * 0.8
            _results_queue.put({"type": "progress", "value": progress, "log": f"({i+1}/{total_steps}) {model_name} ëª¨ë¸ í›ˆë ¨/íŠœë‹ ì¤‘..."})
            start_time = time.time()

            # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
            model_instance = None
            param_dist = None # param_dist ì´ˆê¸°í™”
            if model_name in models_to_tune:  # íŠœë‹ ëŒ€ìƒ ëª¨ë¸
                model_config = models_to_tune[model_name]
                model_class = model_config["model_class"]
                model_instance = model_class(random_state=42)
                param_dist = model_config["params"]
            else:  # ê¸°ë³¸ ëª¨ë¸
                model_instance = base_models_to_run[model_name]

            pipeline = Pipeline(steps=[
                ('preprocessor', preprocessor),
                ('scaler', StandardScaler(with_mean=False)),
                ('classifier' if task_type == 'Classification' else 'regressor', model_instance)
            ])

            # RandomizedSearchCV ë˜ëŠ” ì¼ë°˜ fit ìˆ˜í–‰
            best_pipeline = None
            best_params = {}
            cv_score = 0.0

            if model_name in models_to_tune:
                search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=cv_folds, scoring=sklearn_metric_name, random_state=42, n_jobs=2)
                search.fit(X, y)
                best_pipeline = search.best_estimator_
                cv_score = search.best_score_
                best_params = search.best_params_
                model_display_name = f"Tuned_{model_name.replace(' ', '')}"
            else:  # ê¸°ë³¸ ëª¨ë¸ì€ CV ì ìˆ˜ë§Œ ê³„ì‚°
                from sklearn.model_selection import cross_val_score
                cv_scores = cross_val_score(pipeline, X, y, cv=cv_folds, scoring=sklearn_metric_name)
                cv_score = np.mean(cv_scores)
                pipeline.fit(X_train, y_train)  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ fitì€ í•„ìš”
                best_pipeline = pipeline
                best_params = {"default": "default"}
                model_display_name = model_name
            
            training_time = time.time() - start_time

            # 4. ëª¨ë“  í‰ê°€ì§€í‘œ ê³„ì‚°
            test_metrics = _calculate_all_metrics(best_pipeline, X_test, y_test, task_type)

            # 5. ê²°ê³¼ ì €ì¥
            results.append({
                "id": str(uuid.uuid4()),
                "name": model_display_name,
                "cv_score": cv_score, # RandomizedSearchCV ë˜ëŠ” cross_val_score ê²°ê³¼
                "primary_metric_key": optimization_metric, # ì •ë ¬ ê¸°ì¤€ì´ ë  í‚¤ (e.g., F1)
                "time": training_time,
                "pipeline": best_pipeline,
                "params": best_params,
                "test_metrics": test_metrics, # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì§€í‘œê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬
                "task_info": {"type": task_type, "target": target_name, "le_map": le_map,
                              "X_train": X_train, "y_train": y_train, "X_test": X_test, "y_test": y_test}
            })

        _results_queue.put({"type": "discovery_result", "data": results})

    except Exception as e:
        error_msg = f"ìë™ ëª¨ë¸ íƒìƒ‰ ì˜¤ë¥˜: {str(e)}\n{traceback.format_exc()}"
        _results_queue.put({"type": "error", "log": error_msg})

# --- UI Creation ---
def create_ui(step_name: str, parent_container_tag: str, main_callbacks: dict):
    global _module_main_callbacks, _util_funcs
    _module_main_callbacks = main_callbacks
    _util_funcs = main_callbacks.get('get_util_funcs', lambda: {})()

    main_callbacks['register_step_group_tag'](step_name, TAG_S11_GROUP)

    with dpg.group(tag=TAG_S11_GROUP, parent=parent_container_tag, show=False):
        dpg.add_text(f"--- {step_name} ---")
        dpg.add_separator()
        
        with dpg.tab_bar(tag=TAG_S11_MAIN_TAB_BAR):
            with dpg.tab(label="ëª¨ë¸ë§ (Modeling)", tag=TAG_S11_MODELING_TAB):
                _create_modeling_tab_ui()
            with dpg.tab(label="ì¶”ë¡  (Inference)", tag=TAG_S11_INFERENCE_TAB):
                _create_inference_tab_ui()

    main_callbacks['register_module_updater'](step_name, update_ui)

def _create_modeling_tab_ui():
    # ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë¶€ëª¨ ê·¸ë£¹
    with dpg.group(tag="s11_modeling_main_group"):
        # --- ìƒë‹¨ íŒ¨ë„: ì¢Œìš° ë¶„í•  (ì œì–´íŒ + ê²°ê³¼ì°½) ---
        with dpg.group(horizontal=True):
            # --- ì¢Œì¸¡ ì œì–´íŒ ---
            with dpg.group(width=400): # ë„ˆë¹„ë¥¼ 400ìœ¼ë¡œ ì†Œí­ ì¡°ì •
                dpg.add_text("1. ì„¤ì • (Setup)", color=(255, 255, 0))
                dpg.add_separator()
                dpg.add_text("ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ:")
                dpg.add_combo(label="", tag=TAG_S11_DF_SELECTOR, width=-1, callback=_on_df_or_target_selected)
                dpg.add_text("íƒ€ê²Ÿ ë³€ìˆ˜ (y) ì„ íƒ:")
                dpg.add_combo(label="", tag=TAG_S11_TARGET_SELECTOR, width=-1, callback=_on_df_or_target_selected)
                with dpg.group(horizontal=True):
                    dpg.add_text("ê°ì§€ëœ íƒœìŠ¤í¬ íƒ€ì…:")
                    dpg.add_text("(ë°ì´í„° ì„ íƒ)", tag=TAG_S11_TASK_TYPE_TEXT, color=(255, 255, 0))

                dpg.add_separator()
                dpg.add_text("2. ìë™ íƒìƒ‰ ì„¤ì • (Automated Discovery)", color=(255, 255, 0))
                dpg.add_text("êµì°¨ ê²€ì¦ (CV) í´ë“œ ìˆ˜:")
                dpg.add_combo(items=["3", "5", "10"], default_value="5", width=-1, tag="s11_cv_folds_selector")
                
                # [ìˆ˜ì •] ê¸°ë³¸ ìµœì í™” ì§€í‘œë¥¼ ì„ íƒí•˜ëŠ” UI. ì‹¤ì œ í‘œì‹œëŠ” ëª¨ë“  ì§€í‘œê°€ ë‚˜ì˜´.
                dpg.add_text("ì •ë ¬ ê¸°ì¤€ ì§€í‘œ (Primary Metric):")
                dpg.add_combo(items=["F1", "Accuracy", "Recall", "Precision", "AUC", "Kappa"], default_value="F1", label="ë¶„ë¥˜", width=-1, tag="s11_clf_metric_selector")
                dpg.add_combo(items=["R2", "MAE", "MSE"], default_value="R2", label="íšŒê·€", width=-1, tag="s11_reg_metric_selector", show=False)

                dpg.add_spacer(height=10)
                dpg.add_button(label="ğŸš€ ìµœì  ëª¨ë¸ ìë™ íƒìƒ‰ ì‹¤í–‰", tag=TAG_S11_RUN_AUTO_DISCOVERY_BUTTON, width=-1, height=40,
                               callback=_start_automated_discovery_callback)

            # --- ìš°ì¸¡ ê²°ê³¼ì°½ ---
            with dpg.group():
                # ë¦¬ë”ë³´ë“œ
                with dpg.group(tag=TAG_S11_LEADERBOARD_GROUP):
                    with dpg.group(horizontal=True):
                        dpg.add_text("ëª¨ë¸ ì„±ëŠ¥ ë¦¬ë”ë³´ë“œ", color=(255, 255, 0))
                        dpg.add_spacer()
                        dpg.add_button(label="ğŸ”„", callback=_update_leaderboard_display)
                        dpg.add_button(label=" Ensemble", tag=TAG_S11_ENSEMBLE_BUTTON, show=False, callback=_create_ensemble_callback)
                    # ë¦¬ë”ë³´ë“œ í…Œì´ë¸” ë†’ì´ë¥¼ -1ë¡œ ì„¤ì •í•˜ì—¬ ê°€ë³€ì ìœ¼ë¡œ ë§Œë“¦
                    dpg.add_table(tag=TAG_S11_LEADERBOARD_TABLE, header_row=True, resizable=True,
                                  borders_innerV=True, borders_outerH=True, height=300,
                                  policy=dpg.mvTable_SizingStretchSame)

                dpg.add_separator()
                # ì‹¬ì¸µ ë¶„ì„ ì˜ì—­ (ë¦¬ë”ë³´ë“œ ì•„ë˜ì— ìœ„ì¹˜í•˜ê²Œ ë¨)
                with dpg.group(tag=TAG_S11_DEEP_DIVE_GROUP, show=False):
                    dpg.add_text("ì‹¬ì¸µ ë¶„ì„ ë° ì‚¬ìš©ì ì •ì˜ (Deep Dive & Customization)", color=(255, 255, 0))
                    _create_deep_dive_ui()

        dpg.add_separator()
        # --- í•˜ë‹¨ íŒ¨ë„: ì „ì²´ ë„ˆë¹„ ë¡œê·¸ ì°½ ---
        with dpg.group():
            dpg.add_text("ì§„í–‰ ìƒí™© ë° ë¡œê·¸", color=(100, 200, 255))
            dpg.add_progress_bar(tag=TAG_S11_PROGRESS_BAR, default_value=0.0, width=-1)
            # ë¡œê·¸ ì°½ì˜ ë†’ì´ë¥¼ 150ìœ¼ë¡œ ê³ ì •, ë„ˆë¹„ëŠ” í™”ë©´ ì „ì²´
            with dpg.child_window(height=150, tag=TAG_S11_LOG_WINDOW, border=True):
                dpg.add_input_text(default_value="ìë™ íƒìƒ‰ì„ ì‹œì‘í•˜ì„¸ìš”.", tag=TAG_S11_LOG_TEXT, multiline=True, readonly=True, width=-1, height=-1)


def _create_deep_dive_ui():
    """Track 2: ì‹¬ì¸µ ë¶„ì„ ë° ì‚¬ìš©ì ì •ì˜ UIë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    with dpg.child_window(border=True):
        dpg.add_text("Selected Model: ", tag="s11_deep_dive_model_name")
        dpg.add_separator()
        with dpg.tab_bar():
            with dpg.tab(label="ëª¨ë¸ ì¸ì‚¬ì´íŠ¸ (Insights)"):
                with dpg.group(horizontal=True):
                    dpg.add_text("ë¶„ì„í•  í”¼ì²˜ ì„ íƒ:")
                    dpg.add_combo(tag="s11_insight_feature_selector", width=200)
                with dpg.group(horizontal=True):
                    dpg.add_button(label="SHAP ìš”ì•½ í”Œë¡¯ (Beeswarm)", callback=lambda: _run_shap_analysis("summary"))
                    dpg.add_button(label="SHAP ì˜ì¡´ì„± í”Œë¡¯", callback=lambda: _run_shap_analysis("dependence"))
                dpg.add_separator()
                dpg.add_text("ê°œë³„ ì˜ˆì¸¡ ë¶„ì„ (ë°ì´í„° í…Œì´ë¸”ì—ì„œ í–‰ ì„ íƒ í•„ìš”)")
                dpg.add_button(label="ì„ íƒëœ ë°ì´í„° ì˜ˆì¸¡ ê·¼ê±° ë³´ê¸° (SHAP Force)", callback=lambda: _run_shap_analysis("force"))

                with dpg.child_window(tag="s11_insight_plots_window", border=True):
                    dpg.add_text("ë¶„ì„ ê²°ê³¼ë¥¼ ì—¬ê¸°ì— í‘œì‹œí•©ë‹ˆë‹¤.")

            with dpg.tab(label="íŒŒë¼ë¯¸í„° íŠœë‹ (Tuning)"):
                dpg.add_text("ì´ê³³ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ê³  ì¬í›ˆë ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (êµ¬í˜„ ì˜ˆì •)")
                # ì‚¬ìš©ì ì •ì˜ íŠœë‹ UIê°€ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤.

            with dpg.tab(label="ì„±ëŠ¥ (Performance)"):
                dpg.add_text("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥")
                with dpg.table(tag="s11_deep_dive_perf_table", header_row=True):
                    dpg.add_table_column(label="Metric")
                    dpg.add_table_column(label="Value")
                dpg.add_text("Confusion Matrix ë“± ì‹œê°í™”ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤. (êµ¬í˜„ ì˜ˆì •)")

def _create_inference_tab_ui():
    """ì¶”ë¡  íƒ­ UIë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    dpg.add_text("í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.", wrap=500)
    dpg.add_separator()
    dpg.add_button(label="ğŸ’¾ ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°", callback=_load_model_for_inference)
    dpg.add_text("ë¶ˆëŸ¬ì˜¨ ëª¨ë¸: ì—†ìŒ", tag="s11_inference_model_path")
    dpg.add_separator()
    dpg.add_button(label="ğŸ“„ ì˜ˆì¸¡í•  ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°", callback=_load_data_for_inference)
    dpg.add_text("ì˜ˆì¸¡í•  ë°ì´í„°: ì—†ìŒ", tag="s11_inference_data_path")
    dpg.add_table(tag="s11_inference_data_preview", header_row=True, height=150)
    dpg.add_separator()
    dpg.add_button(label="ì‹¤í–‰", width=-1, callback=_run_inference, height=30)
    dpg.add_separator()
    dpg.add_text("ì˜ˆì¸¡ ê²°ê³¼")
    with dpg.group(horizontal=True):
        dpg.add_text("", tag="s11_inference_result_count")
        dpg.add_button(label="ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", show=False, tag="s11_inference_download_button", callback=_download_inference_result)
    dpg.add_table(tag="s11_inference_result_table", header_row=True, height=200)

# --- Callbacks & UI Update Functions ---
def _on_df_or_target_selected(sender, app_data, user_data):
    """ë°ì´í„° ì†ŒìŠ¤ ë˜ëŠ” íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ ì‹œ UI ì—…ë°ì´íŠ¸"""
    df_name = dpg.get_value(TAG_S11_DF_SELECTOR)
    target_name = dpg.get_value(TAG_S11_TARGET_SELECTOR)
    all_dfs = _module_main_callbacks.get('get_all_available_dfs', lambda: {})()
    if not df_name or df_name not in all_dfs:
        return

    df = all_dfs[df_name]
    if sender == TAG_S11_DF_SELECTOR:
        cols = [""] + df.columns.tolist()
        dpg.configure_item(TAG_S11_TARGET_SELECTOR, items=cols)
        dpg.set_value(TAG_S11_TARGET_SELECTOR, "")

    if target_name and target_name in df.columns:
        y = df[target_name]
        task_type = "Regression"
        if y.dtype == 'object' or y.nunique() < 25:
             task_type = "Classification"
        dpg.set_value(TAG_S11_TASK_TYPE_TEXT, f"{task_type}")
        # íƒœìŠ¤í¬ íƒ€ì…ì— ë”°ë¼ ë©”íŠ¸ë¦­ ì½¤ë³´ë°•ìŠ¤ í‘œì‹œ/ìˆ¨ê¹€
        dpg.configure_item("s11_clf_metric_selector", show=(task_type == "Classification"))
        dpg.configure_item("s11_reg_metric_selector", show=(task_type == "Regression"))
    else:
        dpg.set_value(TAG_S11_TASK_TYPE_TEXT, "(íƒ€ê²Ÿ ì„ íƒ)")

def _start_automated_discovery_callback():
    """'ìµœì  ëª¨ë¸ ìë™ íƒìƒ‰ ì‹¤í–‰' ë²„íŠ¼ ì½œë°±"""
    global _leaderboard_results
    df_name = dpg.get_value(TAG_S11_DF_SELECTOR)
    target_name = dpg.get_value(TAG_S11_TARGET_SELECTOR)
    if not df_name or not target_name:
        _util_funcs['_show_simple_modal_message']("ì„¤ì • ì˜¤ë¥˜", "ë°ì´í„° ì†ŒìŠ¤ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ëª¨ë‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    # ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
    cv_folds = int(dpg.get_value("s11_cv_folds_selector"))
    task_type = dpg.get_value(TAG_S11_TASK_TYPE_TEXT)
    metric_selector = "s11_clf_metric_selector" if task_type == "Classification" else "s11_reg_metric_selector"
    optimization_metric = dpg.get_value(metric_selector)

    all_dfs = _module_main_callbacks.get('get_all_available_dfs')()
    df = all_dfs.get(df_name)
    if df is None:
        _util_funcs['_show_simple_modal_message']("ì˜¤ë¥˜", "ì„ íƒëœ ë°ì´í„°í”„ë ˆì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ìƒíƒœ ì´ˆê¸°í™”
    _leaderboard_results.clear()
    _update_leaderboard_display()
    dpg.configure_item(TAG_S11_DEEP_DIVE_GROUP, show=False)
    dpg.configure_item(TAG_S11_ENSEMBLE_BUTTON, show=False)

    _start_background_task(_run_automated_discovery_thread, args=(df.copy(), target_name, cv_folds, optimization_metric))

def _update_leaderboard_display():
    """ë¦¬ë”ë³´ë“œ í…Œì´ë¸” UIë¥¼ í˜„ì¬ _leaderboard_results ê¸°ì¤€ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨"""
    table = TAG_S11_LEADERBOARD_TABLE
    dpg.delete_item(table, children_only=True)

    if not _leaderboard_results:
        dpg.add_table_column(label="ì•Œë¦¼", parent=table)
        with dpg.table_row(parent=table):
            dpg.add_text("ìë™ íƒìƒ‰ì„ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # ì •ë ¬ ê¸°ì¤€(Primary Metric)ì— ë”°ë¼ ê²°ê³¼ ì •ë ¬
    primary_metric = _leaderboard_results[0].get("primary_metric_key", "F1")
    sorted_results = sorted(
        _leaderboard_results,
        key=lambda x: x.get("test_metrics", {}).get(primary_metric, 0),
        reverse=True
    )
    
    # í—¤ë”(ì»¬ëŸ¼ëª…) ìƒì„±
    metric_keys = list(sorted_results[0].get("test_metrics", {}).keys())
    header_keys = ['Model'] + metric_keys # 'Model' ì»¬ëŸ¼ì„ ë§¨ ì•ì— ì¶”ê°€
    
    best_scores = {}
    for key in metric_keys:
        scores = [res.get("test_metrics", {}).get(key, -np.inf) for res in sorted_results]
        best_scores[key] = max(scores)

    for key in header_keys:
        dpg.add_table_column(label=key, parent=table)
    dpg.add_table_column(label="ë¶„ì„/ì €ì¥", parent=table)

    highlight_color = (255, 255, 150, 100)
    
    for row_idx, res in enumerate(sorted_results):
        with dpg.table_row(parent=table):
            # 1. ëª¨ë¸ ì´ë¦„ í‘œì‹œ (ìˆ˜ì •ëœ í•µì‹¬)
            with dpg.table_cell():
                dpg.add_text(res.get("name", "N/A"))
            
            # 2. ë‚˜ë¨¸ì§€ ë©”íŠ¸ë¦­ í‘œì‹œ
            metrics = res.get("test_metrics", {})
            for col_idx, key in enumerate(metric_keys):
                value = metrics.get(key, "N/A")
                with dpg.table_cell():
                    if isinstance(value, float):
                        cell_text = f"{value:.4f}"
                        dpg.add_text(cell_text)
                        if key in best_scores and abs(value - best_scores[key]) < 1e-6:
                            # ëª¨ë¸ ì´ë¦„ ì»¬ëŸ¼ì´ ë¹ ì¡Œìœ¼ë¯€ë¡œ col_idxëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥
                            dpg.highlight_table_cell(table, row_idx, col_idx + 1, color=highlight_color)
                    else:
                        dpg.add_text(str(value))
            
            with dpg.table_cell():
                 with dpg.group(horizontal=True):
                    dpg.add_button(label="ìƒì„¸ ë¶„ì„", user_data=res["id"], callback=_select_model_for_deep_dive)
                    dpg.add_button(label="ğŸ’¾", user_data=res["id"], callback=_export_model_callback)

    can_ensemble = len([r for r in _leaderboard_results if 'Tuned' in r['name']]) >= 2
    dpg.configure_item(TAG_S11_ENSEMBLE_BUTTON, show=can_ensemble)

def _select_model_for_deep_dive(sender, app_data, user_data_model_id):
    """ë¦¬ë”ë³´ë“œì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ì‹¬ì¸µ ë¶„ì„ UIë¥¼ í™œì„±í™”"""
    global _current_deep_dive_model
    model_data = next((res for res in _leaderboard_results if res["id"] == user_data_model_id), None)
    if not model_data:
        _log_message(f"ì˜¤ë¥˜: ëª¨ë¸ ID {user_data_model_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ERROR")
        return

    _current_deep_dive_model = model_data
    dpg.configure_item(TAG_S11_DEEP_DIVE_GROUP, show=True)
    dpg.set_value("s11_deep_dive_model_name", f"Selected Model: {model_data['name']}")

    # ì¸ì‚¬ì´íŠ¸ íƒ­ í”¼ì²˜ ì½¤ë³´ë°•ìŠ¤ ì±„ìš°ê¸°
    X_train = model_data['task_info']['X_train']
    dpg.configure_item("s11_insight_feature_selector", items=X_train.columns.tolist())

    # ì„±ëŠ¥ íƒ­ ì±„ìš°ê¸°
    perf_table = "s11_deep_dive_perf_table"
    dpg.delete_item(perf_table, children_only=True)
    dpg.add_table_column(label="Metric", parent=perf_table)
    dpg.add_table_column(label="Value", parent=perf_table)
    for metric, value in model_data['test_metrics'].items():
        with dpg.table_row(parent=perf_table):
            dpg.add_text(metric)
            dpg.add_text(f"{value:.4f}")
    
    _log_message(f"'{model_data['name']}' ëª¨ë¸ì´ ì‹¬ì¸µ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

def _export_model_callback(sender, app_data, user_data_model_id):
    """ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì½œë°±"""
    model_data = next((res for res in _leaderboard_results if res["id"] == user_data_model_id), None)
    if not model_data:
        _util_funcs['_show_simple_modal_message']("ì˜¤ë¥˜", "ì €ì¥í•  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # íŒŒì¼ ì €ì¥ ë‹¤ì´ì–¼ë¡œê·¸ (main_app.pyì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ íŒŒì¼ëª…ì„ ì •í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    try:
        save_dir = "trained_models"
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{model_data['name']}_{timestamp}.joblib"
        save_path = os.path.join(save_dir, filename)

        joblib.dump(model_data['pipeline'], save_path)
        _util_funcs['_show_simple_modal_message']("ì €ì¥ ì™„ë£Œ", f"ëª¨ë¸ì´ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{save_path}")
        _log_message(f"ëª¨ë¸ '{model_data['name']}'ì´(ê°€) '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        _util_funcs['_show_simple_modal_message']("ì €ì¥ ì‹¤íŒ¨", f"ëª¨ë¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
        _log_message(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}", "ERROR")

# --- ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° ë° í—¬í¼ í•¨ìˆ˜ ---
def _calculate_metrics(y_true, y_pred, task_type):
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if task_type == "Classification":
        return {
            "Accuracy": accuracy_score(y_true, y_pred),
            "F1 Score (Weighted)": f1_score(y_true, y_pred, average='weighted', zero_division=0),
            "Recall (Weighted)": recall_score(y_true, y_pred, average='weighted', zero_division=0),
            "Precision (Weighted)": precision_score(y_true, y_pred, average='weighted', zero_division=0),
        }
    else: # Regression
        return {
            "R2 Score": r2_score(y_true, y_pred),
            "Mean Squared Error (MSE)": mean_squared_error(y_true, y_pred),
            "Mean Absolute Error (MAE)": mean_absolute_error(y_true, y_pred),
        }

def _start_background_task(target_func, args):
    """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¥¼ ì‹œì‘í•˜ëŠ” ë˜í¼ í•¨ìˆ˜"""
    global _worker_thread
    if _worker_thread and _worker_thread.is_alive():
        _util_funcs['_show_simple_modal_message']("ì‘ì—… ì¤‘", "ì´ì „ ì‘ì—…ì´ ì•„ì§ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return
    _worker_thread = threading.Thread(target=target_func, args=args, daemon=True)
    _worker_thread.start()
    _update_progress(0.0, "ì‘ì—…ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤...")

def _check_for_updates():
    """ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ íë¥¼ í™•ì¸í•˜ê³  UIë¥¼ ì—…ë°ì´íŠ¸"""
    global _worker_thread, _leaderboard_results
    try:
        result = _results_queue.get_nowait()
        
        if result["type"] == "progress":
            _update_progress(result["value"], result["log"])
        elif result["type"] == "error":
            _log_message(result["log"], "ERROR")
            _update_progress(0.0, "ì‘ì—… ì‹¤íŒ¨.")
        elif result["type"] == "discovery_result":
            _leaderboard_results = result["data"]
            _update_leaderboard_display()
            _update_progress(1.0, "ìë™ íƒìƒ‰ ì™„ë£Œ.")
            time.sleep(1)
            _update_progress(0.0)
        
        # ë‹¤ë¥¸ result type (e.g., shap_result) ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

        _results_queue.task_done()
    except queue.Empty:
        pass
    except Exception as e:
        print(f"ì—…ë°ì´íŠ¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n{traceback.format_exc()}")

    if _worker_thread and not _worker_thread.is_alive():
        _worker_thread = None

def _update_progress(value: float, message: str = ""):
    """ì§„í–‰ ë°” ë° ë¡œê·¸ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸"""
    if not dpg.is_dearpygui_running(): return
    if dpg.does_item_exist(TAG_S11_PROGRESS_BAR):
        dpg.set_value(TAG_S11_PROGRESS_BAR, value)
    if message: 
        _log_message(message)

def _log_message(message: str, level: str = "INFO"):
    """ë¡œê·¸ ì°½ì— ë©”ì‹œì§€ ì¶”ê°€"""
    if not dpg.is_dearpygui_running() or not dpg.does_item_exist(TAG_S11_LOG_TEXT): return
    timestamp = datetime.datetime.now().strftime("%H:%M:%S")
    formatted_msg = f"[{timestamp}] {level}: {message}"
    
    current_log = dpg.get_value(TAG_S11_LOG_TEXT)
    if "ìë™ íƒìƒ‰ì„ ì‹œì‘í•˜ì„¸ìš”." in current_log:
        current_log = ""

    new_log = f"{current_log}\n{formatted_msg}"
    log_lines = new_log.split("\n")
    if len(log_lines) > 100: 
        new_log = "\n".join(log_lines[-100:])
        
    dpg.set_value(TAG_S11_LOG_TEXT, new_log)
    if dpg.does_item_exist(TAG_S11_LOG_WINDOW):
        dpg.set_y_scroll(TAG_S11_LOG_WINDOW, dpg.get_y_scroll_max(TAG_S11_LOG_WINDOW))

# --- main_app.pyì™€ì˜ ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜ ---
def update_ui():
    """main_appì—ì„œ í˜¸ì¶œí•˜ì—¬ UI ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸ (e.g., ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡)"""
    if not _module_main_callbacks: return
    all_dfs = _module_main_callbacks.get('get_all_available_dfs', lambda: {})()
    if dpg.does_item_exist(TAG_S11_DF_SELECTOR):
        df_names = [""] + list(all_dfs.keys())
        dpg.configure_item(TAG_S11_DF_SELECTOR, items=df_names)

def reset_state():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¦¬ì…‹ ì‹œ í˜¸ì¶œë˜ì–´ ì´ ëª¨ë“ˆì˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”"""
    global _worker_thread, _leaderboard_results, _texture_tags, _current_deep_dive_model
    if _worker_thread and _worker_thread.is_alive():
        return # ì‘ì—… ì¤‘ ë¦¬ì…‹ ë°©ì§€

    _leaderboard_results.clear()
    _current_deep_dive_model = None
    for tag in _texture_tags:
        if dpg.does_item_exist(tag): 
            dpg.delete_item(tag)
    _texture_tags.clear()
    
    if dpg.is_dearpygui_running():
        _update_leaderboard_display()
        dpg.configure_item(TAG_S11_DEEP_DIVE_GROUP, show=False)
        dpg.set_value(TAG_S11_MAIN_TAB_BAR, TAG_S11_MODELING_TAB)
        update_ui()
    
    _log_message("ML ëª¨ë¸ë§ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", "INFO")

# --- ì•„ë˜ëŠ” ì‹¬ì¸µ ë¶„ì„, ì•™ìƒë¸”, ì¶”ë¡  ë“± ì¶”ê°€ êµ¬í˜„ì´ í•„ìš”í•œ ê¸°ëŠ¥ë“¤ì˜ ìë¦¬ì…ë‹ˆë‹¤ ---

def _create_ensemble_callback():
    _log_message("ì•™ìƒë¸” ê¸°ëŠ¥ì€ í˜„ì¬ êµ¬í˜„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.", "INFO")
    # TODO: ìƒìœ„ ëª¨ë¸ 2-3ê°œë¥¼ ì„ íƒí•˜ì—¬ VotingClassifier/Regressorë¥¼ ë§Œë“¤ê³ 
    # _leaderboard_resultsì— ì¶”ê°€í•œ í›„ _update_leaderboard_display() í˜¸ì¶œ

def _run_shap_analysis(plot_type: str):
    _log_message(f"SHAP ë¶„ì„ ({plot_type}) ê¸°ëŠ¥ì€ í˜„ì¬ êµ¬í˜„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.", "INFO")
    # TODO: _current_deep_dive_modelì„ ê¸°ë°˜ìœ¼ë¡œ SHAP ë¶„ì„ ìŠ¤ë ˆë“œ ì‹¤í–‰
    # 1. Explainer ìƒì„±
    # 2. shap_values ê³„ì‚°
    # 3. plot_typeì— ë§ëŠ” í”Œë¡¯ ìƒì„± (matplotlib)
    # 4. plot_to_dpg_texture ìœ í‹¸ë¦¬í‹°ë¡œ DPG í…ìŠ¤ì²˜ ë³€í™˜
    # 5. s11_insight_plots_windowì— ì´ë¯¸ì§€ ì¶”ê°€

def _load_model_for_inference():
    _log_message("ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥ì€ íŒŒì¼ ë‹¤ì´ì–¼ë¡œê·¸ ì—°ë™ í›„ êµ¬í˜„ë©ë‹ˆë‹¤.", "INFO")

def _load_data_for_inference():
    _log_message("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ê¸°ëŠ¥ì€ íŒŒì¼ ë‹¤ì´ì–¼ë¡œê·¸ ì—°ë™ í›„ êµ¬í˜„ë©ë‹ˆë‹¤.", "INFO")

def _run_inference():
    _log_message("ì¶”ë¡  ì‹¤í–‰ ê¸°ëŠ¥ì€ ëª¨ë¸/ë°ì´í„° ë¡œë”© êµ¬í˜„ í›„ í™œì„±í™”ë©ë‹ˆë‹¤.", "INFO")

def _download_inference_result():
    _log_message("ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ì€ ì¶”ë¡  ì‹¤í–‰ êµ¬í˜„ í›„ í™œì„±í™”ë©ë‹ˆë‹¤.", "INFO")